<!DOCTYPE html>
<html lang="en">

  <head>
    
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  M√©d√©ric  Fourmy


</title>
<meta name="description" content="Mederic's professional website">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ü§ñ</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="http://0.0.0.0:4000/">


<!-- Dark Mode -->
<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              about
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
            <div class="toggle-container">
              <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     <span class="font-weight-bold">M√©d√©ric</span>  Fourmy
    </h1>
     <p class="desc"></p>
  </header>

  <article>
    

    <div class="clearfix">
      <p>I am a Postdoc between <a href="https://www.laas.fr/public/" target="_blank" rel="noopener noreferrer">LAAS-CNRS</a> in <a href="https://gepettoweb.laas.fr/" target="_blank" rel="noopener noreferrer">Gepetto team</a> working with <a href="https://gepettoweb.laas.fr/index.php/Members/NicolasMansard" target="_blank" rel="noopener noreferrer">Nicolas Mansard</a> and <a href="https://www.iri.upc.edu/facilities" target="_blank" rel="noopener noreferrer">IRI</a> working with
<a href="http://www.joansola.eu/" target="_blank" rel="noopener noreferrer">Joan Sol√†</a>.</p>

<p>Main main research topic is at the interface between factor graph state estimation and legged robotics. I proposed new formulations
for measurement models of proprioceptive and exteroceptive sensors and implemented solutions on quadruped and humanoid robots. The goal
of this project is to fuse all sensor modalities in a single tightly coupled estimator to provide a globally consistent robot pose estimation
that can be used for both estimation and planning.</p>

<p>This video presents our work on tightly coupled base and centroidal state estimation for legged robots:</p>
<iframe width="560" height="315" sandbox="allow-same-origin allow-scripts allow-popups" src="https://peertube.laas.fr/videos/embed/16822d27-3557-4e35-9a0d-ce5b0aea4c27" frameborder="0" allowfullscreen=""></iframe>

<p>I also collaborated with Cesar Debeunne to build a object level Visual Inertial SLAM system based on <a href="https://www.di.ens.fr/willow/research/cosypose/" target="_blank" rel="noopener noreferrer">CosyPose</a>.</p>

<iframe width="560" height="315" sandbox="allow-same-origin allow-scripts allow-popups" src="https://peertube.laas.fr/videos/embed/56edb26d-e2c3-46ac-909b-61f55d10c569" frameborder="0" allowfullscreen=""></iframe>

<p>I received an aerospace engineering degree from <a href="https://www.isae-supaero.fr/en/" target="_blank" rel="noopener noreferrer">ISAE Supaero</a> with speciality in data science, robotics and operational research. I completed my Master Thesis as part of the <a href="http://osl.eps.hw.ac.uk/index.php" target="_blank" rel="noopener noreferrer">Ocean System Laboratory</a>, working on visual servoing and force control
under the supervision of <a href="https://www.edinburgh-robotics.org/academics/yvan-petillot" target="_blank" rel="noopener noreferrer">Yvan Petillot</a> at Heriot Watt University, Edinburgh, UK.</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/ZkgKq-Zxk3w" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

    </div>

    

    
      <div class="publications">
  <h2>Publications</h2>
  <ol class="bibliography">
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">AbsoluteLoca</abbr>
    
  
  </div>

  <div id="fourmy2019absolute" class="col-sm-8">
    
      <div class="title">Absolute humanoid localization and mapping based on IMU Lie group and fiducial markers</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Fourmy, M√©d√©ric</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Atchuthan, Dinesh,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Mansard, Nicolas,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Sola, Joan,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Flayols, Thomas
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In IEEE-RAS 19th International Conference on Humanoid Robots (Humanoids)</em>
      
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      <a href="https://ieeexplore.ieee.org/document/9035005" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
    
    
      
      <a href="https://hal.archives-ouvertes.fr/hal-02183498/document" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Current locomotion algorithms in structured (indoor) 3D environments require an accurate localization. The
several and diverse sensors typically embedded on legged robots
(IMU, coders, vision and/or LIDARS) should make it possible
if properly fused. Yet this is a difficult task due to the heterogeneity of these sensors and the real-time requirement of the
control. While previous works were using staggered approaches
(odometry at high frequency, sparsely corrected from vision and
LIDAR localization), the recent progress in optimal estimation,
in particular in visual-inertial localization, is paving the way to
a holistic fusion. This paper is a contribution in this direction.
We propose to quantify how a visual-inertial navigation system
can accurately localize a humanoid robot in a 3D indoor
environment tagged with fiducial markers. We introduce a theoretical contribution strengthening the formulation of Forster‚Äôs
IMU pre-integration, a practical contribution to avoid possible
ambiguity raised by pose estimation of fiducial markers, and an
experimental contribution on a humanoid dataset with ground
truth. Our system is able to localize the robot with less than 2 cm
errors once the environment is properly mapped. This would
naturally extend to additional measurements corresponding to
leg odometry (kinematic factors) thanks to the genericity of the
proposed pre-integration algebra.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ContactPreint</abbr>
    
  
  </div>

  <div id="fourmy2021contact" class="col-sm-8">
    
      <div class="title">Contact Forces Preintegration for Estimation in Legged Robotics using Factor Graphs</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Fourmy, M√©d√©ric</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Flayols, Thomas,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  L√©ziart, Pierre-Alexandre,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Mansard, Nicolas,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Sol√†, Joan
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In IEEE International Conference on Robotics and Automation (ICRA)</em>
      
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      <a href="https://ieeexplore.ieee.org/abstract/document/9561037" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
    
    
      
      <a href="https://hal.archives-ouvertes.fr/hal-02991717/document" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>State estimation, in particular estimation of the
base position, orientation and velocity, plays a big role in
the efficiency of legged robot stabilization. The estimation of
the base state is particularly important because of its strong
correlation with the underactuated dynamics, i.e. the evolution
of center of mass and angular momentum. Yet this estimation
is typically done in two phases, first estimating the base state,
then reconstructing the center of mass from the robot model.
The underactuated dynamics is indeed not properly observed,
and any bias in the model would not be corrected from
the sensors. While it has already been observed that force
measurements make such a bias observable, these are often
only used for a binary estimation of the contact state. In this
paper, we propose to simultaneously estimate the base and the
underactuation state by using all measurements simultaneously.
To this end, we propose several contributions to implement a
complete state estimator using factor graphs. Contact forces
altering the underactuated dynamics are pre-integrated using
a novel adaptation of the IMU pre-integration method, which
constitutes the principal contribution. IMU pre-integration is
also used to measure the positional motion of the base. Encoder
measurements are then participating to the estimation in two
ways: by providing leg odometry displacements, contributing to
the observability of IMU biases; and by relating the positional
and centroidal states, thus connecting the whole graph and
producing a tightly-coupled whole-body estimator. The validity
of the approach is demonstrated on real data captured by the
Solo12 quadruped robot</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">WOLF</abbr>
    
  
  </div>

  <div id="sola2021wolf" class="col-sm-8">
    
      <div class="title">WOLF: A modular estimation framework for robotics based on factor graphs</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Sola, Joan,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Vallve, Joan,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Casals, Joaquim,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Deray, Jeremie,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Fourmy, M√©d√©ric</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Atchuthan, Dinesh,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Corominas-Murtra, Andreu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Andrade-Cetto, Juan
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>IEEE Robotics and Automation Letters</em>
      
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2110.12919.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper introduces WOLF, a C++ estimation
framework based on factor graphs and targeted at mobile
robotics. WOLF extends the applications of factor graphs from
the typical problems of SLAM and odometry to a general
estimation framework able to handle self-calibration, model
identification, or the observation of dynamic quantities other
than localization. WOLF produces high throughput estimates
at sensor rates up to the kHz range, which can be used for
feedback control of highly dynamic robots such as humanoids,
quadrupeds or aerial manipulators. Departing from the factor
graph paradigm, the architecture of WOLF allows for a modular
yet tightly-coupled estimator. Modularity is based on plugins
that are loaded at runtime. Then, integration is achieved simply
through YAML files, allowing users to configure a wide range
of applications without the need of writing or compiling code.
Synchronization of incoming data and their processing into
a unique factor graph is achieved through a decentralized
strategy of frame creation and joining. Most algorithmic assets
are coded as abstract algorithms in base classes with varying
levels of specialization. Overall, these assets allow for coherent
processing and favor code reusability and scalability. WOLF can
be interfaced with different solvers, and we provide a wrapper
to Google Ceres. Likewise, we offer ROS integration, providing
a generic ROS node and specialized packages with subscribers
and publishers. WOLF is made publicly available and open to
collaboration.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">CosySLAM</abbr>
    
  
  </div>

  <div id="debeunne:hal-03351438" class="col-sm-8">
    
      <div class="title">CosySlam: investigating object-level SLAM for detecting locomotion surfaces</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Debeunne, C√©sar,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Fourmy, M√©d√©ric</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Labb√©, Yann,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  L√©ziart, Pierre-Alexandre,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Saurel, Guilhem,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Sol√†, Joan,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Mansard, Nicolas
                
              
            
          
        
      </div>

      <div class="periodical">
      
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://hal.archives-ouvertes.fr/hal-03351438v2/document" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>While blindfolded legged locomotion has demonstrated impressive capabilities in the last few years, further progresses are expected from using exteroceptive perception to better adapt the robot behavior to the available surfaces of contact. In this paper, we investigate whether mono cameras are suitable sensors for that aim. We propose to rely on object-level SLAM, fusing RGB images and inertial measurements, to simultaneously estimate the robot balance state (orientation in the gravity field and velocity), the robot position, and the location of candidate contact surfaces. We used CosyPose, a learning-based object pose estimator for which we propose an empirical uncertainty model, as the sole front-end of our visual inertial SLAM. We then combine it with inertial measurements which ideally complete the system observability, although extending the proposed approach would be straightforward (e.g. kinematic information about the contact, or a feature based visual front end). We demonstrate the interest of object-based SLAM on several locomotion sequences, by some absolute metrics and in comparison with other mono SLAM.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
</ol>
</div>

    

    
    <div class="social">
      <div class="contact-icons">
        <a href="mailto:%6D%65%64%65%72%69%63.%66%6F%75%72%6D%79@%6C%61%61%73.%66%72" title="email"><i class="fas fa-envelope"></i></a>

<a href="https://scholar.google.com/citations?user=eBMti0cAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a>


<a href="https://github.com/MedericFourmy" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a>
<a href="https://www.linkedin.com/in/mederic-fourmy-738aa3110" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a>














      </div>
      <div class="contact-note">reach me via email: mederic.fourmy@laas.fr
</div>
    </div>
    
  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    ¬© Copyright 2022 M√©d√©ric  Fourmy.
    Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.

    
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

  
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  





</html>
