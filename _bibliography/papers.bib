---
---


@inproceedings{fourmy2019absolute,
  abbr={AbsoluteLoca},
  title={Absolute humanoid localization and mapping based on IMU Lie group and fiducial markers},
  author={Fourmy, M{\'e}d{\'e}ric and Atchuthan, Dinesh and Mansard, Nicolas and Sola, Joan and Flayols, Thomas},
  abstract={Current locomotion algorithms in structured (indoor) 3D environments require an accurate localization. The
several and diverse sensors typically embedded on legged robots
(IMU, coders, vision and/or LIDARS) should make it possible
if properly fused. Yet this is a difficult task due to the heterogeneity of these sensors and the real-time requirement of the
control. While previous works were using staggered approaches
(odometry at high frequency, sparsely corrected from vision and
LIDAR localization), the recent progress in optimal estimation,
in particular in visual-inertial localization, is paving the way to
a holistic fusion. This paper is a contribution in this direction.
We propose to quantify how a visual-inertial navigation system
can accurately localize a humanoid robot in a 3D indoor
environment tagged with fiducial markers. We introduce a theoretical contribution strengthening the formulation of Forsterâ€™s
IMU pre-integration, a practical contribution to avoid possible
ambiguity raised by pose estimation of fiducial markers, and an
experimental contribution on a humanoid dataset with ground
truth. Our system is able to localize the robot with less than 2 cm
errors once the environment is properly mapped. This would
naturally extend to additional measurements corresponding to
leg odometry (kinematic factors) thanks to the genericity of the
proposed pre-integration algebra.},
  booktitle={2019 IEEE-RAS 19th International Conference on Humanoid Robots (Humanoids)},
  pages={237--243},
  year={2019},
  organization={IEEE},
  html={https://ieeexplore.ieee.org/document/9035005},
  pdf={https://hal.archives-ouvertes.fr/hal-02183498/document},
  selected={true}
}


@inproceedings{fourmy2021contact,
  abbr={ContactPreint},
  title={Contact Forces Preintegration for Estimation in Legged Robotics using Factor Graphs},
  author={Fourmy, M{\'e}d{\'e}ric and Flayols, Thomas and L{\'e}ziart, Pierre-Alexandre and Mansard, Nicolas and Sol{\`a}, Joan},
  abstract={State estimation, in particular estimation of the
base position, orientation and velocity, plays a big role in
the efficiency of legged robot stabilization. The estimation of
the base state is particularly important because of its strong
correlation with the underactuated dynamics, i.e. the evolution
of center of mass and angular momentum. Yet this estimation
is typically done in two phases, first estimating the base state,
then reconstructing the center of mass from the robot model.
The underactuated dynamics is indeed not properly observed,
and any bias in the model would not be corrected from
the sensors. While it has already been observed that force
measurements make such a bias observable, these are often
only used for a binary estimation of the contact state. In this
paper, we propose to simultaneously estimate the base and the
underactuation state by using all measurements simultaneously.
To this end, we propose several contributions to implement a
complete state estimator using factor graphs. Contact forces
altering the underactuated dynamics are pre-integrated using
a novel adaptation of the IMU pre-integration method, which
constitutes the principal contribution. IMU pre-integration is
also used to measure the positional motion of the base. Encoder
measurements are then participating to the estimation in two
ways: by providing leg odometry displacements, contributing to
the observability of IMU biases; and by relating the positional
and centroidal states, thus connecting the whole graph and
producing a tightly-coupled whole-body estimator. The validity
of the approach is demonstrated on real data captured by the
Solo12 quadruped robot},
  booktitle={2021 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={1372--1378},
  year={2021},
  organization={IEEE},
  html={https://ieeexplore.ieee.org/abstract/document/9561037},
  pdf={https://hal.archives-ouvertes.fr/hal-02991717/document},
  selected={true}
}


@article{sola2021wolf,
  abbr={WOLF},
  title={WOLF: A modular estimation framework for robotics based on factor graphs},
  author={Sola, Joan and Vallve, Joan and Casals, Joaquim and Deray, Jeremie and Fourmy, M{\'e}d{\'e}ric and Atchuthan, Dinesh and Corominas-Murtra, Andreu and Andrade-Cetto, Juan},
  journal={IEEE Robotics and Automation Letters},
  year={2022},
  publisher={IEEE},
  abstract={This paper introduces WOLF, a C++ estimation
framework based on factor graphs and targeted at mobile
robotics. WOLF extends the applications of factor graphs from
the typical problems of SLAM and odometry to a general
estimation framework able to handle self-calibration, model
identification, or the observation of dynamic quantities other
than localization. WOLF produces high throughput estimates
at sensor rates up to the kHz range, which can be used for
feedback control of highly dynamic robots such as humanoids,
quadrupeds or aerial manipulators. Departing from the factor
graph paradigm, the architecture of WOLF allows for a modular
yet tightly-coupled estimator. Modularity is based on plugins
that are loaded at runtime. Then, integration is achieved simply
through YAML files, allowing users to configure a wide range
of applications without the need of writing or compiling code.
Synchronization of incoming data and their processing into
a unique factor graph is achieved through a decentralized
strategy of frame creation and joining. Most algorithmic assets
are coded as abstract algorithms in base classes with varying
levels of specialization. Overall, these assets allow for coherent
processing and favor code reusability and scalability. WOLF can
be interfaced with different solvers, and we provide a wrapper
to Google Ceres. Likewise, we offer ROS integration, providing
a generic ROS node and specialized packages with subscribers
and publishers. WOLF is made publicly available and open to
collaboration.},
  pdf={https://arxiv.org/pdf/2110.12919.pdf},
  selected={true},
}


@unpublished{debeunne:hal-03351438,
  abbr={CosySLAM},
  TITLE = {CosySLAM: tracking contact features using visual-inertial object-level SLAM for locomotion},
  AUTHOR = {Debeunne, C{\'e}sar and Fourmy, M{\'e}d{\'e}ric and Labb{\'e}, Yann and L{\'e}ziart, Pierre-Alexandre and Saurel, Guilhem and Sol{\`a}, Joan and Mansard, Nicolas},
  abstract={A legged robot is equipped with several sensors
observing different classes of information, in order to provide
various estimates on its states and its environment. While state
estimation and mapping in this domain have traditionally been
investigated through multiple local filters, recent progresses
have been made toward tightly-coupled estimation. Multiple
observations are then merged into an a-posteriori maximum
estimating several quantities that otherwise were separately
estimated. With this paper, our goal is to move one step further,
by leveraging on object-based simultaneous localization and
mapping. We use an object pose estimator to localize the relative
placement of the robot with respect to large elements of the
environments, e.g. stair steps. These measurements are merged
with other typical observations of legged robots, e.g. inertial
measurements, to provide an estimation of the robot state
(position, orientation and velocity of the basis) along with an
accurate estimation of the environment pieces. It then provides
a consistent estimation of these two quantities, which is an
important property as both would be needed to control the
robot locomotion. We provide a complete implementation of
this idea with the object tracker CosyPose, which we trained
on our environment and for which we provide a covariance
model, and with the SLAM engine Wolf used as a visual-inertial
estimator on the quadruped robot Solo.},
  URL = {https://hal.archives-ouvertes.fr/hal-03351438},
  journal={adzadazazzadaz},
  NOTE = {working paper or preprint},
  HAL_LOCAL_REFERENCE = {Rapport LAAS n{\textdegree} 21255},
  PDF = {https://hal.archives-ouvertes.fr/hal-03351438/file/CosySLAM_ICRA_2022.pdf},
  HAL_ID = {hal-03351438},
  HAL_VERSION = {v1},
  selected={true}
}
