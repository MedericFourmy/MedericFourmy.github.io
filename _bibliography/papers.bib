---
---


@inproceedings{fourmy2019absolute,
  abbr={AbsoluteLoca},
  title={Absolute humanoid localization and mapping based on IMU Lie group and fiducial markers},
  author={Fourmy, M{\'e}d{\'e}ric and Atchuthan, Dinesh and Mansard, Nicolas and Sola, Joan and Flayols, Thomas},
  abstract={Current locomotion algorithms in structured (indoor) 3D environments require an accurate localization. The
several and diverse sensors typically embedded on legged robots
(IMU, coders, vision and/or LIDARS) should make it possible
if properly fused. Yet this is a difficult task due to the heterogeneity of these sensors and the real-time requirement of the
control. While previous works were using staggered approaches
(odometry at high frequency, sparsely corrected from vision and
LIDAR localization), the recent progress in optimal estimation,
in particular in visual-inertial localization, is paving the way to
a holistic fusion. This paper is a contribution in this direction.
We propose to quantify how a visual-inertial navigation system
can accurately localize a humanoid robot in a 3D indoor
environment tagged with fiducial markers. We introduce a theoretical contribution strengthening the formulation of Forsterâ€™s
IMU pre-integration, a practical contribution to avoid possible
ambiguity raised by pose estimation of fiducial markers, and an
experimental contribution on a humanoid dataset with ground
truth. Our system is able to localize the robot with less than 2 cm
errors once the environment is properly mapped. This would
naturally extend to additional measurements corresponding to
leg odometry (kinematic factors) thanks to the genericity of the
proposed pre-integration algebra.},
  booktitle={IEEE-RAS 19th International Conference on Humanoid Robots (Humanoids)},
  pages={237--243},
  year={2019},
  organization={IEEE},
  html={https://ieeexplore.ieee.org/document/9035005},
  pdf={https://hal.archives-ouvertes.fr/hal-02183498/document},
  selected={true}
}


@inproceedings{fourmy2021contact,
  abbr={ContactPreint},
  title={Contact Forces Preintegration for Estimation in Legged Robotics using Factor Graphs},
  author={Fourmy, M{\'e}d{\'e}ric and Flayols, Thomas and L{\'e}ziart, Pierre-Alexandre and Mansard, Nicolas and Sol{\`a}, Joan},
  abstract={State estimation, in particular estimation of the
base position, orientation and velocity, plays a big role in
the efficiency of legged robot stabilization. The estimation of
the base state is particularly important because of its strong
correlation with the underactuated dynamics, i.e. the evolution
of center of mass and angular momentum. Yet this estimation
is typically done in two phases, first estimating the base state,
then reconstructing the center of mass from the robot model.
The underactuated dynamics is indeed not properly observed,
and any bias in the model would not be corrected from
the sensors. While it has already been observed that force
measurements make such a bias observable, these are often
only used for a binary estimation of the contact state. In this
paper, we propose to simultaneously estimate the base and the
underactuation state by using all measurements simultaneously.
To this end, we propose several contributions to implement a
complete state estimator using factor graphs. Contact forces
altering the underactuated dynamics are pre-integrated using
a novel adaptation of the IMU pre-integration method, which
constitutes the principal contribution. IMU pre-integration is
also used to measure the positional motion of the base. Encoder
measurements are then participating to the estimation in two
ways: by providing leg odometry displacements, contributing to
the observability of IMU biases; and by relating the positional
and centroidal states, thus connecting the whole graph and
producing a tightly-coupled whole-body estimator. The validity
of the approach is demonstrated on real data captured by the
Solo12 quadruped robot},
  booktitle={IEEE International Conference on Robotics and Automation (ICRA)},
  pages={1372--1378},
  year={2021},
  organization={IEEE},
  html={https://ieeexplore.ieee.org/abstract/document/9561037},
  pdf={https://hal.archives-ouvertes.fr/hal-02991717/document},
  selected={true}
}


@article{sola2021wolf,
  abbr={WOLF},
  title={WOLF: A modular estimation framework for robotics based on factor graphs},
  author={Sola, Joan and Vallve, Joan and Casals, Joaquim and Deray, Jeremie and Fourmy, M{\'e}d{\'e}ric and Atchuthan, Dinesh and Corominas-Murtra, Andreu and Andrade-Cetto, Juan},
  journal={IEEE Robotics and Automation Letters},
  year={2022},
  publisher={IEEE},
  abstract={This paper introduces WOLF, a C++ estimation
framework based on factor graphs and targeted at mobile
robotics. WOLF extends the applications of factor graphs from
the typical problems of SLAM and odometry to a general
estimation framework able to handle self-calibration, model
identification, or the observation of dynamic quantities other
than localization. WOLF produces high throughput estimates
at sensor rates up to the kHz range, which can be used for
feedback control of highly dynamic robots such as humanoids,
quadrupeds or aerial manipulators. Departing from the factor
graph paradigm, the architecture of WOLF allows for a modular
yet tightly-coupled estimator. Modularity is based on plugins
that are loaded at runtime. Then, integration is achieved simply
through YAML files, allowing users to configure a wide range
of applications without the need of writing or compiling code.
Synchronization of incoming data and their processing into
a unique factor graph is achieved through a decentralized
strategy of frame creation and joining. Most algorithmic assets
are coded as abstract algorithms in base classes with varying
levels of specialization. Overall, these assets allow for coherent
processing and favor code reusability and scalability. WOLF can
be interfaced with different solvers, and we provide a wrapper
to Google Ceres. Likewise, we offer ROS integration, providing
a generic ROS node and specialized packages with subscribers
and publishers. WOLF is made publicly available and open to
collaboration.},
  pdf={https://arxiv.org/pdf/2110.12919.pdf},
  selected={true},
}


@unpublished{debeunne:hal-03351438,
  abbr={CosySLAM},
  TITLE = {CosySlam: investigating object-level SLAM for detecting locomotion surfaces},
  AUTHOR = {Debeunne, C{\'e}sar and Fourmy, M{\'e}d{\'e}ric and Labb{\'e}, Yann and L{\'e}ziart, Pierre-Alexandre and Saurel, Guilhem and Sol{\`a}, Joan and Mansard, Nicolas},
  abstract={While blindfolded legged locomotion has demonstrated impressive capabilities in the last few years, further progresses are expected from using exteroceptive perception to better adapt the robot behavior to the available surfaces of contact. In this paper, we investigate whether mono cameras are suitable sensors for that aim. We propose to rely on object-level SLAM, fusing RGB images and inertial measurements, to simultaneously estimate the robot balance state (orientation in the gravity field and velocity), the robot position, and the location of candidate contact surfaces. We used CosyPose, a learning-based object pose estimator for which we propose an empirical uncertainty model, as the sole front-end of our visual inertial SLAM. We then combine it with inertial measurements which ideally complete the system observability, although extending the proposed approach would be straightforward (e.g. kinematic information about the contact, or a feature based visual front end). We demonstrate the interest of object-based SLAM on several locomotion sequences, by some absolute metrics and in comparison with other mono SLAM.},
  URL = {https://hal.archives-ouvertes.fr/hal-03351438},
  journal={},
  NOTE = {working paper or preprint},
  HAL_LOCAL_REFERENCE = {Rapport LAAS n{\textdegree} 21255},
  PDF = {https://hal.archives-ouvertes.fr/hal-03351438v2/document},
  HAL_ID = {hal-03351438},
  HAL_VERSION = {v1},
  selected={true}
}
